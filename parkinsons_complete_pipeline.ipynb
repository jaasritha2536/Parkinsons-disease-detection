{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7fc402e",
   "metadata": {},
   "source": [
    "\n",
    "# Parkinson’s Disease Detection Using Machine Learning\n",
    "\n",
    "This notebook implements an **end-to-end machine learning pipeline** to detect Parkinson’s disease using biomedical voice measurements.\n",
    "\n",
    "## Project Objective (Problem Statement)\n",
    "\n",
    "Parkinson’s disease is a progressive neurological disorder that affects movement and speech. Early detection helps in better treatment and quality of life. In this project, we use the **UCI Parkinson’s dataset**, which contains various voice-related biomedical features extracted from sustained phonation. The **target variable** is `status` (1 = Parkinson’s, 0 = healthy). The goal is to build and evaluate classification models that can accurately distinguish between Parkinson’s and healthy subjects based on their voice features.  \n",
    "We follow a complete ML workflow: data loading, exploratory data analysis (EDA), preprocessing, model building, feature engineering, hyperparameter tuning, evaluation, and model deployment (via a saved model and Streamlit app).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3659a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model building\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report, RocCurveDisplay\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Interpretability\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Persisting model\n",
    "import joblib\n",
    "\n",
    "# Display settings\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b9e290",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Load the Dataset\n",
    "\n",
    "We load the **Parkinson’s dataset** (`parkinsons.csv`) from the project folder and inspect its basic structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5bbd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset (ensure parkinsons.csv is in the same folder as this notebook)\n",
    "data_path = \"parkinsons.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(\"Shape of dataset:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22ce71f",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Dataset Overview & Data Quality Checks\n",
    "\n",
    "We check:\n",
    "- Column types  \n",
    "- Missing values  \n",
    "- Basic statistics  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e716abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic info\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39c7100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check missing values\n",
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a838244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Statistical summary of numeric columns\n",
    "df.describe().T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea3e263",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 3.1 Target Variable Distribution (`status`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea03b5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Target distribution\n",
    "target_counts = df['status'].value_counts().sort_index()\n",
    "print(target_counts)\n",
    "\n",
    "plt.figure()\n",
    "sns.countplot(x=\"status\", data=df)\n",
    "plt.title(\"Distribution of Target Variable (status)\")\n",
    "plt.xlabel(\"status (0 = healthy, 1 = Parkinson's)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d202aaf",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2 Feature Distributions\n",
    "\n",
    "We visualize distributions of a few representative numerical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9ddade",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "numeric_cols = [col for col in numeric_cols if col != \"status\"]\n",
    "\n",
    "# Plot distributions for first 6 numeric features (for brevity)\n",
    "sample_features = numeric_cols[:6]\n",
    "\n",
    "for col in sample_features:\n",
    "    plt.figure()\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dcadbc",
   "metadata": {},
   "source": [
    "\n",
    "### 3.3 Feature–Target Relationship\n",
    "\n",
    "We use boxplots to see how some features differ between Parkinson’s and healthy subjects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in sample_features:\n",
    "    plt.figure()\n",
    "    sns.boxplot(x=\"status\", y=col, data=df)\n",
    "    plt.title(f\"{col} by status\")\n",
    "    plt.xlabel(\"status (0 = healthy, 1 = Parkinson's)\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97be2645",
   "metadata": {},
   "source": [
    "\n",
    "### 3.4 Correlation Analysis\n",
    "\n",
    "We check correlations between features and the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28306911",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation of features with target\n",
    "corr_target = corr[\"status\"].sort_values(ascending=False)\n",
    "corr_target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eb13fc",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Data Preprocessing & Train–Test Split\n",
    "\n",
    "We drop non-feature identifier columns (like `name` if present), separate features and target, split into training and test sets, and apply **standardization**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13489d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop non-numeric identifier column (e.g., 'name') if present\n",
    "if \"name\" in df.columns:\n",
    "    df_model = df.drop(columns=[\"name\"])\n",
    "else:\n",
    "    df_model = df.copy()\n",
    "\n",
    "X = df_model.drop(columns=[\"status\"])\n",
    "y = df_model[\"status\"]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e37569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69c62f1",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Baseline Model Building & Evaluation\n",
    "\n",
    "We start with three baseline models:\n",
    "\n",
    "1. **Logistic Regression**  \n",
    "2. **K-Nearest Neighbors (KNN)**  \n",
    "3. **Support Vector Machine (SVM)**  \n",
    "\n",
    "We compare them using Accuracy, Precision, Recall, F1-score, and ROC-AUC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f83a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_classifier(name, model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Train the model, make predictions, and return a dictionary of metrics.\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = None\n",
    "    try:\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    except Exception:\n",
    "        try:\n",
    "            y_scores = model.decision_function(X_test)\n",
    "            y_prob = (y_scores - y_scores.min()) / (y_scores.max() - y_scores.min())\n",
    "        except Exception:\n",
    "            y_prob = None\n",
    "\n",
    "    metrics = {\n",
    "        \"model\": name,\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_test, y_pred, zero_division=0),\n",
    "    }\n",
    "    if y_prob is not None:\n",
    "        metrics[\"roc_auc\"] = roc_auc_score(y_test, y_prob)\n",
    "    else:\n",
    "        metrics[\"roc_auc\"] = np.nan\n",
    "    return metrics, model, y_pred, y_prob\n",
    "\n",
    "results = []\n",
    "\n",
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_res, log_model, log_pred, log_prob = evaluate_classifier(\n",
    "    \"Logistic Regression\", log_reg, X_train_scaled, y_train, X_test_scaled, y_test\n",
    ")\n",
    "results.append(log_res)\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_res, knn_model, knn_pred, knn_prob = evaluate_classifier(\n",
    "    \"KNN\", knn, X_train_scaled, y_train, X_test_scaled, y_test\n",
    ")\n",
    "results.append(knn_res)\n",
    "\n",
    "# SVM (RBF kernel)\n",
    "svm = SVC(kernel=\"rbf\", probability=True, random_state=42)\n",
    "svm_res, svm_model, svm_pred, svm_prob = evaluate_classifier(\n",
    "    \"SVM (RBF)\", svm, X_train_scaled, y_train, X_test_scaled, y_test\n",
    ")\n",
    "results.append(svm_res)\n",
    "\n",
    "baseline_results = pd.DataFrame(results)\n",
    "baseline_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f89429",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot baseline model comparison\n",
    "metrics_to_plot = [\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"]\n",
    "baseline_results_melted = baseline_results.melt(id_vars=\"model\", value_vars=metrics_to_plot,\n",
    "                                                var_name=\"metric\", value_name=\"score\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=baseline_results_melted, x=\"metric\", y=\"score\", hue=\"model\")\n",
    "plt.title(\"Baseline Model Comparison\")\n",
    "plt.ylim(0, 1.05)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c505334",
   "metadata": {},
   "source": [
    "\n",
    "### 5.1 Confusion Matrix & ROC Curve for Best Baseline Model\n",
    "\n",
    "We take the best baseline model (likely SVM) and visualize its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26af0a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_baseline_name = baseline_results.sort_values(\"f1\", ascending=False).iloc[0][\"model\"]\n",
    "best_baseline_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a7ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use SVM as the main candidate (you can change this if another model performs better)\n",
    "best_model = svm_model\n",
    "best_pred = svm_pred\n",
    "best_prob = svm_prob\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, best_pred)\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Baseline SVM\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report (Baseline SVM):\")\n",
    "print(classification_report(y_test, best_pred))\n",
    "\n",
    "# ROC curve\n",
    "if best_prob is not None:\n",
    "    RocCurveDisplay.from_predictions(y_test, best_prob)\n",
    "    plt.title(\"ROC Curve - Baseline SVM\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e94346",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Hyperparameter Tuning (SVM)\n",
    "\n",
    "We tune `C` and `gamma` for the SVM model using **GridSearchCV** and compare default vs tuned performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2333fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid = {\n",
    "    \"C\": [0.1, 1, 10, 100],\n",
    "    \"gamma\": [0.01, 0.1, 1, 10],\n",
    "    \"kernel\": [\"rbf\"]\n",
    "}\n",
    "\n",
    "svm_base = SVC(probability=True, random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    svm_base,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV F1 score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d793ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate tuned SVM on test set\n",
    "svm_tuned = grid_search.best_estimator_\n",
    "tuned_res, tuned_model, tuned_pred, tuned_prob = evaluate_classifier(\n",
    "    \"SVM (Tuned)\", svm_tuned, X_train_scaled, y_train, X_test_scaled, y_test\n",
    ")\n",
    "\n",
    "tuned_results = pd.DataFrame([svm_res, tuned_res])\n",
    "tuned_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ebe0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visual comparison: baseline vs tuned SVM\n",
    "tuned_results_melted = tuned_results.melt(id_vars=\"model\", value_vars=metrics_to_plot,\n",
    "                                          var_name=\"metric\", value_name=\"score\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=tuned_results_melted, x=\"metric\", y=\"score\", hue=\"model\")\n",
    "plt.title(\"Baseline vs Tuned SVM Performance\")\n",
    "plt.ylim(0, 1.05)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Confusion matrix and ROC for tuned SVM\n",
    "cm_tuned = confusion_matrix(y_test, tuned_pred)\n",
    "plt.figure()\n",
    "sns.heatmap(cm_tuned, annot=True, fmt=\"d\", cmap=\"Greens\")\n",
    "plt.title(\"Confusion Matrix - Tuned SVM\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report (Tuned SVM):\")\n",
    "print(classification_report(y_test, tuned_pred))\n",
    "\n",
    "if tuned_prob is not None:\n",
    "    RocCurveDisplay.from_predictions(y_test, tuned_prob)\n",
    "    plt.title(\"ROC Curve - Tuned SVM\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357912ff",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Model Interpretability (Permutation Importance)\n",
    "\n",
    "We use **permutation importance** to understand which features are most important for the tuned SVM model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f293dd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r = permutation_importance(\n",
    "    tuned_model, X_test_scaled, y_test, n_repeats=20, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"importance_mean\": r.importances_mean,\n",
    "    \"importance_std\": r.importances_std,\n",
    "}).sort_values(\"importance_mean\", ascending=False)\n",
    "\n",
    "importance_df.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a00dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot top 15 important features\n",
    "top_n = 15\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(\n",
    "    data=importance_df.head(top_n),\n",
    "    x=\"importance_mean\",\n",
    "    y=\"feature\"\n",
    ")\n",
    "plt.title(\"Top Feature Importances (Permutation Importance)\")\n",
    "plt.xlabel(\"Mean Importance Decrease\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24fe0b1",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Error Analysis\n",
    "\n",
    "We inspect some of the misclassified examples to understand where the model fails.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7155d3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add predictions to a copy of X_test\n",
    "error_df = X_test.copy()\n",
    "error_df[\"true_status\"] = y_test\n",
    "error_df[\"pred_status\"] = tuned_pred\n",
    "error_df[\"correct\"] = error_df[\"true_status\"] == error_df[\"pred_status\"]\n",
    "\n",
    "# Show first few misclassified samples\n",
    "misclassified = error_df[~error_df[\"correct\"]]\n",
    "misclassified.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd9d4d9",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Save Final Model & Scaler for Deployment\n",
    "\n",
    "We save the tuned SVM model and scaler using `joblib`. These files will be used in the **Streamlit app**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0802f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save model and scaler\n",
    "joblib.dump(tuned_model, \"parkinsons_model.pkl\")\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "print(\"Saved: parkinsons_model.pkl and scaler.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cbf242",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Conclusion\n",
    "\n",
    "- We built an **end-to-end ML pipeline** for Parkinson’s disease detection using voice features.  \n",
    "- After EDA and preprocessing (scaling), we trained **Logistic Regression**, **KNN**, and **SVM** models.  \n",
    "- The **SVM model** gave the best baseline performance.  \n",
    "- Using **GridSearchCV**, we tuned the SVM hyperparameters and improved key metrics such as **F1-score** and **ROC-AUC** on the test set.  \n",
    "- **Permutation importance** helped identify the most influential features in the prediction.  \n",
    "- Finally, we saved the tuned model and scaler for deployment via a **Streamlit web app**.\n",
    "\n",
    "This notebook satisfies the full ML project lifecycle: problem definition, EDA, model building, evaluation, interpretability, and deployment readiness.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
